{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from pathlib import Path\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/univai-ss2019/SoLong/runs/0ne3fc0l\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init()\n",
    "\n",
    "config = run.config\n",
    "\n",
    "bb_params = ['height', 'width', 'x', 'y']\n",
    "config.width = 224 # or 299\n",
    "config.height = 224 # or 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bb(bb, size):\n",
    "    bb = [bb[p] for p in bb_params]\n",
    "    conv_x = (config.width / size[0])\n",
    "    conv_y = (config.height / size[1])\n",
    "    bb[0] = bb[0]*conv_y\n",
    "    bb[1] = bb[1]*conv_x\n",
    "    bb[2] = max(bb[2]*conv_x, 0)\n",
    "    bb[3] = max(bb[3]*conv_y, 0)\n",
    "    return bb\n",
    "\n",
    "\n",
    "def create_rect(bb, color='red'):\n",
    "    return plt.Rectangle((bb[2], bb[3]), bb[1], bb[0], color=color, fill=False, lw=3)\n",
    "\n",
    "def to_plot(img):\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        return np.rollaxis(img, 0, 1).astype(np.uint8)\n",
    "    else:\n",
    "        return np.rollaxis(img, 0, 3).astype(np.uint8)\n",
    "\n",
    "def plotfish(img):\n",
    "    plt.imshow(to_plot(img))\n",
    "    \n",
    "def show_bb(i):\n",
    "    bb = val_bbox[i]\n",
    "    plotfish(val[i])\n",
    "    plt.gca().add_patch(create_rect(bb))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "folders = glob.glob('train/*')\n",
    "from sklearn.model_selection import train_test_split\n",
    "anno_classes = ['ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT', 'NOF']\n",
    "annodict = dict(enumerate(anno_classes))\n",
    "dictanno = {v: k for k, v in annodict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for folder in folders:\n",
    "    files = glob.glob(folder+\"/*.jpg\")\n",
    "    labels = [e.split('/')[1] for e in files]\n",
    "    train, valid = train_test_split(range(len(files)), test_size=0.2, random_state=1983)\n",
    "    mask = np.zeros(len(files))\n",
    "    for j in train:\n",
    "        mask[j] = 1\n",
    "    for i, label in enumerate(labels):\n",
    "        d = dict(label=dictanno[label], file=files[i], train=mask[i])\n",
    "        records.append(d)\n",
    "import pandas as pd\n",
    "\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bbox/ALB_labels.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9c7126b98c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manno_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bbox/{}_labels.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'annotations'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bbox/ALB_labels.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "bb_json = {}\n",
    "\n",
    "for c in anno_classes:\n",
    "    j = json.load(open('bbox/{}_labels.json'.format(c), 'r'))\n",
    "    for l in j:\n",
    "        if 'annotations' in l.keys() and len(l['annotations'])>0:\n",
    "            bb_json[l['filename'].split('/')[-1]] = sorted(\n",
    "                l['annotations'], key=lambda x: x['height']*x['width'])[-1]\n",
    "print(bb_json.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bbox/alb_labels.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-83ec231e3b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0manno_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'alb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dol'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'other'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shark'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yft'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NoF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manno_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bbox/{}_labels.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'annotations'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bbox/alb_labels.json'"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "tot=0\n",
    "keys = bb_json.keys()\n",
    "records2 = []\n",
    "for r in records:\n",
    "    tot +=1\n",
    "    name = r['file'].split('/')[-1]\n",
    "    if not name in keys:\n",
    "        count += 1\n",
    "        #print(r['file'])\n",
    "        r['bbox'] = None\n",
    "    else:\n",
    "        bbox = bb_json[name]\n",
    "        r['x'] = bbox['x']\n",
    "        r['y'] = bbox['y']\n",
    "        r['width'] = bbox['width']\n",
    "        r['height'] = bbox['height']\n",
    "        records2.append(r)\n",
    "print(\"nobbox\", count, tot)\n",
    "print(\"rec5\", records2[:5])\n",
    "df = pd.DataFrame.from_records(records2)\n",
    "dftrain = df[df.train==1][['file', 'label', 'x', 'y', 'width', 'height']]\n",
    "dfvalid = df[df.train==0][['file', 'label', 'x', 'y', 'width', 'height']]\n",
    "\n",
    "dftrain.to_csv(\"tv_train.csv\", index=False, header=True)\n",
    "dfvalid.to_csv(\"tv_valid.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224,224\n",
    "\n",
    "input_shape = (img_width,img_height,3)\n",
    "\n",
    "train_data_dir = '../SoLong/kagfish/train'\n",
    "\n",
    "config.n_train_samples = 200\n",
    "config.n_validation_samples = 200\n",
    "config.epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        validation_split = 0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size = (config.width, config.height),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        subset = \"training\",\n",
    "        classes = anno_classes,\n",
    "        class_mode = 'categorical')\n",
    "\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size = (config.width, config.height),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        subset = \"validation\",\n",
    "        classes = anno_classes,\n",
    "        class_mode = 'categorical')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "#exit()\n",
    "sgd = SGD(lr = 1e-4)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#best_model_file = \"./weights.h5\"\n",
    "#best_model = ModelCheckpoint(best_model_file, monitor='val_acc', verbose = 1, save_best_only = True)\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = config.n_train_samples,\n",
    "        nb_epoch = config.epochs,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = config.n_validation_samples,\n",
    "        callbacks = [WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
